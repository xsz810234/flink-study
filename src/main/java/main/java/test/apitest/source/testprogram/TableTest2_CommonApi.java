package main.java.test.apitest.source.testprogram;

import org.apache.calcite.interpreter.Row;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.tuple.Tuple3;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.BatchTableEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.table.catalog.ResolvedSchema;
import org.apache.flink.table.operations.QueryOperation;

public class TableTest2_CommonApi {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

        DataStream<String> inputStream = env.readTextFile("/Users/wb-xsz810234/Idea/workspace/flink-study/src/main/resources/sensor.txt");
        SingleOutputStreamOperator<Sensor> dataStream = inputStream.map(line -> {
            String[] fields = line.split(" ");
            return new Sensor(fields[0], Long.valueOf(fields[1]), Double.valueOf(fields[2]));
        });
        Table sensorTable = tableEnv.fromDataStream(dataStream);

        ResolvedSchema resolvedSchema = sensorTable.getResolvedSchema();
        QueryOperation queryOperation = sensorTable.getQueryOperation();
        tableEnv.toAppendStream(sensorTable, Sensor.class).print("sensorTable");

        //批处理环境
       // ExecutionEnvironment batchEnv = ExecutionEnvironment.getExecutionEnvironment();
        env.execute();
    }
}
